{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94ec02b",
   "metadata": {},
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HumBug-Mosquito/HumBugDB.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5625aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_1.zip?download=1\n",
    "!wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_2.zip?download=1\n",
    "!wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_3.zip?download=1\n",
    "!wget https://zenodo.org/record/4904800/files/humbugdb_neurips_2021_4.zip?download=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/humbugdb_neurips_2021_1.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "!unzip /content/humbugdb_neurips_2021_2.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "!unzip /content/humbugdb_neurips_2021_3.zip?download=1 -d '/content/HumBugDB/data/audio'\n",
    "!unzip /content/humbugdb_neurips_2021_4.zip?download=1 -d '/content/HumBugDB/data/audio'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c6e844",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define paths using raw strings\n",
    "mosquito_csv_path = r'C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\HumBugDB\\data\\metadata\\neurips_2021_zenodo_0_0_1.csv'\n",
    "noise_csv_path = r'C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\Actual noise for mosquito project\\meta\\esc50_noise.csv'\n",
    "mosquito_audio_dir = r'C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\HumBugDB\\data\\audio'\n",
    "noise_audio_dir = r'C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\Actual noise for mosquito project'\n",
    "\n",
    "# Load mosquito data\n",
    "mosquito_df = pd.read_csv(mosquito_csv_path)\n",
    "mosquito_df = mosquito_df[mosquito_df['sound_type'] == 'mosquito']\n",
    "\n",
    "# Clean species column\n",
    "mosquito_df = mosquito_df[mosquito_df['species'].apply(lambda x: isinstance(x, str))]\n",
    "unique_species = mosquito_df['species'].unique()\n",
    "print(\"Unique species detected:\", unique_species)\n",
    "\n",
    "# Create mapping of species to indices\n",
    "species_to_index = {species: idx + 1 for idx, species in enumerate(unique_species)}\n",
    "species_to_index['No Mosquito'] = 0\n",
    "\n",
    "# List valid mosquito audio files\n",
    "mosquito_files = [f\"{row['id']}.wav\" for _, row in mosquito_df.iterrows() if os.path.exists(os.path.join(mosquito_audio_dir, f\"{row['id']}.wav\"))]\n",
    "\n",
    "# Load noise data\n",
    "noise_df = pd.read_csv(noise_csv_path)\n",
    "noise_files = [row['filename'] for _, row in noise_df.iterrows() if os.path.exists(os.path.join(noise_audio_dir, row['filename']))]\n",
    "\n",
    "if not mosquito_files or not noise_files:\n",
    "    raise FileNotFoundError(\"No valid audio files found. Check paths or download data.\")\n",
    "\n",
    "# Define the Dataset Class\n",
    "class MixedMosquitoDataset(Dataset):\n",
    "    def __init__(self, mosquito_files, noise_files, sample_rate=16000, duration=2):\n",
    "        self.mosquito_files = mosquito_files\n",
    "        self.noise_files = noise_files\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.num_samples = len(mosquito_files) * 2  # Twice the number of mosquito files\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.mosquito_files):\n",
    "            # Mixed sample\n",
    "            mosquito_file = random.choice(self.mosquito_files)\n",
    "            noise_file = random.choice(self.noise_files)\n",
    "\n",
    "            # Check if mosquito file exists\n",
    "            mosquito_file_path = os.path.join(mosquito_audio_dir, mosquito_file)\n",
    "            if not os.path.exists(mosquito_file_path):\n",
    "                print(f\"File not found: {mosquito_file_path}\")\n",
    "                return None, 0  # Return dummy data\n",
    "\n",
    "            mosquito_waveform, _ = torchaudio.load(mosquito_file_path)\n",
    "\n",
    "            # Check if noise file exists\n",
    "            noise_file_path = os.path.join(noise_audio_dir, noise_file)\n",
    "            if not os.path.exists(noise_file_path):\n",
    "                print(f\"File not found: {noise_file_path}\")\n",
    "                return None, 0  # Return dummy data\n",
    "\n",
    "            noise_waveform, _ = torchaudio.load(noise_file_path)\n",
    "\n",
    "            # Ensure both audio files have the same sample rate\n",
    "            if mosquito_waveform.shape[0] != noise_waveform.shape[0]:\n",
    "                noise_waveform = torchaudio.transforms.Resample(orig_freq=noise_waveform.shape[0], new_freq=mosquito_waveform.shape[0])(noise_waveform)\n",
    "\n",
    "            # Adjust lengths\n",
    "            target_length = max(mosquito_waveform.shape[1], noise_waveform.shape[1])\n",
    "            mosquito_waveform = torch.nn.functional.pad(mosquito_waveform, (0, target_length - mosquito_waveform.shape[1]))\n",
    "            noise_waveform = torch.nn.functional.pad(noise_waveform, (0, target_length - noise_waveform.shape[1]))\n",
    "\n",
    "            # Mix audio\n",
    "            mixed_waveform = 0.5 * mosquito_waveform + 0.5 * noise_waveform\n",
    "\n",
    "            # Save mixed audio\n",
    "            output_file_name = f'mixed_{idx}.wav'\n",
    "            output_file_path = os.path.join(audio_dir, output_file_name)\n",
    "            torchaudio.save(output_file_path, mixed_waveform, self.sample_rate)\n",
    "\n",
    "            # Prepare metadata entry\n",
    "            mosquito_id = mosquito_file.split('.')[0]\n",
    "            noise_id = noise_file.split('.')[0]\n",
    "            species = mosquito_df.loc[mosquito_df['id'] == mosquito_id, 'species'].values[0]\n",
    "            return mixed_waveform, {\n",
    "                'id': idx,\n",
    "                'file_name': output_file_name,\n",
    "                'mosquito_id': mosquito_id,\n",
    "                'mosquito_species': species,\n",
    "                'noise_id': noise_id,\n",
    "                'noise_type': noise_df.loc[noise_df['filename'] == noise_file, 'category'].values[0]  # Assuming 'category' column exists\n",
    "            }\n",
    "        else:\n",
    "            # Pure noise sample\n",
    "            noise_file = random.choice(self.noise_files)\n",
    "            noise_waveform, _ = torchaudio.load(os.path.join(noise_audio_dir, noise_file))\n",
    "            noise_waveform = self._adjust_length(noise_waveform)\n",
    "            mixed_waveform = noise_waveform\n",
    "            label = 0  # No Mosquito\n",
    "\n",
    "        # Convert to Mel spectrogram\n",
    "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(n_mels=128, n_fft=2048, hop_length=512)(mixed_waveform)\n",
    "        mel_spectrogram = torchaudio.transforms.AmplitudeToDB()(mel_spectrogram)\n",
    "\n",
    "        return mel_spectrogram.squeeze(0), label\n",
    "\n",
    "    def _adjust_length(self, waveform):\n",
    "        # Adjust length to 2 seconds (sample_rate * duration)\n",
    "        target_length = self.sample_rate * self.duration\n",
    "        if waveform.shape[1] < target_length:\n",
    "            # Pad\n",
    "            padding = target_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "        else:\n",
    "            # Trim\n",
    "            start = random.randint(0, waveform.shape[1] - target_length)\n",
    "            waveform = waveform[:, start:start + target_length]\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adb8f4",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25105850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Mosquito species CNN training (CPU-only) using mixed audios + controlled noise-only negatives.\n",
    "\n",
    "- Reads your metadata CSV (must include: mix_file, species; optional: sound_type)\n",
    "- Uses mix_{id}.wav at AUDIO_OUTPUT_DIR and species labels from CSV\n",
    "- Adds extra negatives from NOISE_DIR as class 0 = \"no_mosquito\" (controlled fraction)\n",
    "- Stratified split with robust fallback if some species are too rare\n",
    "- Class-balanced sampler for training\n",
    "- Early stopping + checkpointing best val accuracy\n",
    "- Final export to:\n",
    "    * PyTorch weights (.pt)\n",
    "    * TorchScript (.pt)\n",
    "    * ONNX (.onnx)\n",
    "\n",
    "Avoids classification_report label mismatch by explicitly passing labels seen in y_true/y_pred.\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# -----------------------\n",
    "# ==== YOUR PATHS ====\n",
    "# -----------------------\n",
    "AUDIO_OUTPUT_DIR = r\"C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\mosquito_noise_mix\"\n",
    "CSV_PATH         = r\"C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\mosquito_noise_mix\\metadata\\esc50_noise.csv\"\n",
    "NOISE_DIR        = r\"C:\\Users\\akash\\OneDrive\\Desktop\\ML projects\\mosquitos\\Actual noise for mosquito project\"\n",
    "\n",
    "# -----------------------\n",
    "# ==== CONFIG ===========\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "SR = 16000\n",
    "DURATION = 2.0            # seconds (will pad/trim to this)\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 160\n",
    "FMIN = 20\n",
    "FMAX = SR // 2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "VAL_SIZE = 0.2\n",
    "NUM_WORKERS = 0           # Windows-safe\n",
    "PIN_MEMORY = False        # CPU-only\n",
    "MAX_NOISE_FRACTION = 0.25 # cap negatives as 25% of total to avoid overfitting\n",
    "\n",
    "MODEL_DIR = os.path.join(AUDIO_OUTPUT_DIR, \"models\")\n",
    "EXPORT_DIR = os.path.join(MODEL_DIR, \"exports\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Force CPU\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# ===== Utilities =======\n",
    "# -----------------------\n",
    "def _safe_exists(fp: str) -> bool:\n",
    "    try:\n",
    "        return os.path.exists(fp)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def list_audio_files(root: str, exts=(\".wav\", \".flac\", \".mp3\")) -> List[str]:\n",
    "    out = []\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for f in filenames:\n",
    "            if f.lower().endswith(exts):\n",
    "                out.append(os.path.join(dirpath, f))\n",
    "    return out\n",
    "\n",
    "def load_audio_fixed(path: str, sr: int, duration: float) -> np.ndarray:\n",
    "    \"\"\"Load audio, resample, mono, pad/trim to fixed duration.\"\"\"\n",
    "    target_len = int(sr * duration)\n",
    "    try:\n",
    "        y, file_sr = librosa.load(path, sr=sr, mono=True)\n",
    "    except Exception:\n",
    "        # fallback to soundfile then resample with librosa\n",
    "        y, file_sr = sf.read(path, always_2d=False)\n",
    "        if y.ndim > 1:\n",
    "            y = np.mean(y, axis=1)\n",
    "        if file_sr != sr:\n",
    "            y = librosa.resample(y, orig_sr=file_sr, target_sr=sr)\n",
    "    if y.size == 0:\n",
    "        y = np.zeros(target_len, dtype=np.float32)\n",
    "    # pad/trim\n",
    "    if len(y) < target_len:\n",
    "        pad = target_len - len(y)\n",
    "        y = np.pad(y, (0, pad), mode=\"constant\")\n",
    "    elif len(y) > target_len:\n",
    "        y = y[:target_len]\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def to_logmelspec(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS, fmin=FMIN, fmax=FMAX, power=2.0\n",
    "    )\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    # per-sample standardization\n",
    "    mu = S_db.mean()\n",
    "    sigma = S_db.std() + 1e-6\n",
    "    S_db = (S_db - mu) / sigma\n",
    "    return S_db.astype(np.float32)  # (n_mels, time)\n",
    "\n",
    "def spec_augment(spec: torch.Tensor, time_mask_prob=0.3, freq_mask_prob=0.3,\n",
    "                 time_mask_max=20, freq_mask_max=12) -> torch.Tensor:\n",
    "    \"\"\"SpecAugment on (1, n_mels, time).\"\"\"\n",
    "    _, n_mels, T = spec.shape\n",
    "    out = spec.clone()\n",
    "    if random.random() < time_mask_prob:\n",
    "        t = random.randint(1, min(time_mask_max, max(1, T // 8)))\n",
    "        t0 = random.randint(0, max(0, T - t))\n",
    "        out[:, :, t0:t0 + t] = 0.0\n",
    "    if random.random() < freq_mask_prob:\n",
    "        f = random.randint(1, min(freq_mask_max, max(1, n_mels // 8)))\n",
    "        f0 = random.randint(0, max(0, n_mels - f))\n",
    "        out[:, f0:f0 + f, :] = 0.0\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# ===== Dataset(s) ======\n",
    "# -----------------------\n",
    "class MixedMosquitoDataset(Dataset):\n",
    "    \"\"\"Samples from mixed audios listed in CSV (mix_file, species).\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, label2idx: Dict[str, int], augment: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.label2idx = label2idx\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = str(row[\"mix_file\"])\n",
    "        species = str(row[\"species\"]).strip()\n",
    "        label = self.label2idx.get(species, None)\n",
    "        if label is None:\n",
    "            # safety: treat unknown as \"no_mosquito\"\n",
    "            label = self.label2idx[\"no_mosquito\"]\n",
    "\n",
    "        y = load_audio_fixed(path, SR, DURATION)\n",
    "        # light waveform jitter\n",
    "        if self.augment:\n",
    "            # random time shift up to 10% length\n",
    "            max_shift = int(0.1 * len(y))\n",
    "            if max_shift > 0 and random.random() < 0.5:\n",
    "                s = random.randint(-max_shift, max_shift)\n",
    "                y = np.roll(y, s)\n",
    "            # tiny gaussian noise\n",
    "            if random.random() < 0.3:\n",
    "                y = y + 0.005 * np.random.randn(len(y)).astype(np.float32)\n",
    "\n",
    "        spec = to_logmelspec(y, SR)  # (mels, T)\n",
    "        spec = torch.from_numpy(spec).unsqueeze(0)  # (1, mels, T)\n",
    "        if self.augment:\n",
    "            spec = spec_augment(spec)\n",
    "        return spec, int(label)\n",
    "\n",
    "class NoiseOnlyDataset(Dataset):\n",
    "    \"\"\"Negative-only samples from NOISE_DIR => class 0 (no_mosquito).\"\"\"\n",
    "    def __init__(self, files: List[str], label_no_mosquito: int, augment: bool = True):\n",
    "        self.files = files\n",
    "        self.label = label_no_mosquito\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        path = self.files[idx]\n",
    "        y = load_audio_fixed(path, SR, DURATION)\n",
    "        if self.augment:\n",
    "            max_shift = int(0.1 * len(y))\n",
    "            if max_shift > 0 and random.random() < 0.5:\n",
    "                s = random.randint(-max_shift, max_shift)\n",
    "                y = np.roll(y, s)\n",
    "            if random.random() < 0.3:\n",
    "                y = y + 0.005 * np.random.randn(len(y)).astype(np.float32)\n",
    "        spec = to_logmelspec(y, SR)\n",
    "        spec = torch.from_numpy(spec).unsqueeze(0)\n",
    "        if self.augment:\n",
    "            spec = spec_augment(spec)\n",
    "        return spec, self.label\n",
    "\n",
    "# -----------------------\n",
    "# ======= Model =========\n",
    "# -----------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, pool=(2, 2)):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(pool)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(1, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# -----------------------\n",
    "# ==== Train / Eval =====\n",
    "# -----------------------\n",
    "def build_label_space(df: pd.DataFrame) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"Build label mapping with class 0 = no_mosquito, others from species present.\"\"\"\n",
    "    species = df[\"species\"].fillna(\"\").astype(str).str.strip()\n",
    "    species = sorted([s for s in species.unique() if s != \"\"])\n",
    "    label2idx = {\"no_mosquito\": 0}\n",
    "    for i, s in enumerate(species, start=1):\n",
    "        label2idx[s] = i\n",
    "    idx2label = {v: k for k, v in label2idx.items()}\n",
    "    return label2idx, idx2label\n",
    "\n",
    "def make_datasets(df_all: pd.DataFrame) -> Tuple[Dataset, Dataset, Dict[str, int], Dict[int, str]]:\n",
    "    # Filter mixed rows that actually exist and have species\n",
    "    mask_exist = df_all[\"mix_file\"].astype(str).apply(_safe_exists)\n",
    "    df = df_all[mask_exist].copy()\n",
    "    # Keep only rows with a species label (these define positive classes)\n",
    "    df = df[df[\"species\"].fillna(\"\").astype(str).str.strip() != \"\"].copy()\n",
    "    if len(df) == 0:\n",
    "        raise RuntimeError(\"No valid mixed samples found with existing 'mix_file' and non-empty 'species'.\")\n",
    "\n",
    "    label2idx, idx2label = build_label_space(df)\n",
    "\n",
    "    # Mixed dataset\n",
    "    mixed_ds = MixedMosquitoDataset(df=df, label2idx=label2idx, augment=True)\n",
    "\n",
    "    # Noise-only dataset (class 0)\n",
    "    noise_files = list_audio_files(NOISE_DIR, exts=(\".wav\", \".flac\"))\n",
    "    random.shuffle(noise_files)\n",
    "    # Cap negatives so they don't exceed MAX_NOISE_FRACTION of total\n",
    "    max_noise = int(MAX_NOISE_FRACTION * len(mixed_ds))\n",
    "    if max_noise < 1 and len(noise_files) > 0:\n",
    "        max_noise = 1\n",
    "    noise_files = noise_files[:max_noise]\n",
    "    noise_ds = NoiseOnlyDataset(noise_files, label_no_mosquito=label2idx[\"no_mosquito\"], augment=True)\n",
    "\n",
    "    # Combined\n",
    "    full_ds = ConcatDataset([mixed_ds, noise_ds])\n",
    "\n",
    "    return full_ds, mixed_ds, label2idx, idx2label\n",
    "\n",
    "def stratified_split_indices(labels: np.ndarray, val_size: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Try stratified; if fails due to rare classes, fall back to random split.\"\"\"\n",
    "    try:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=SEED)\n",
    "        train_idx, val_idx = next(sss.split(np.zeros_like(labels), labels))\n",
    "    except Exception:\n",
    "        warnings.warn(\"Stratified split failed due to rare classes. Falling back to random split.\", RuntimeWarning)\n",
    "        train_idx, val_idx = train_test_split(\n",
    "            np.arange(len(labels)), test_size=val_size, random_state=SEED, shuffle=True\n",
    "        )\n",
    "    return train_idx, val_idx\n",
    "\n",
    "def indices_and_labels_from_concat(ds: ConcatDataset) -> np.ndarray:\n",
    "    \"\"\"Extract labels for each item in ConcatDataset without loading audio.\"\"\"\n",
    "    labels = []\n",
    "    for sub in ds.datasets:\n",
    "        if isinstance(sub, MixedMosquitoDataset):\n",
    "            labs = sub.df[\"species\"].fillna(\"\").astype(str).str.strip().map(lambda s: sub.label2idx.get(s, 0)).tolist()\n",
    "            labels.extend(labs)\n",
    "        elif isinstance(sub, NoiseOnlyDataset):\n",
    "            labels.extend([sub.label] * len(sub))\n",
    "        else:\n",
    "            raise TypeError(\"Unexpected dataset type in ConcatDataset.\")\n",
    "    return np.array(labels, dtype=np.int64)\n",
    "\n",
    "def make_loaders(ds_full: ConcatDataset, train_idx: np.ndarray, val_idx: np.ndarray,\n",
    "                 labels_all: np.ndarray) -> Tuple[DataLoader, DataLoader]:\n",
    "    # Train weights for class-balanced sampling\n",
    "    y_train = labels_all[train_idx]\n",
    "    class_counts = np.bincount(y_train, minlength=y_train.max() + 1)\n",
    "    class_weights = 1.0 / (class_counts + 1e-6)\n",
    "    sample_weights = class_weights[y_train]\n",
    "    # ensure proper dtype for sampler\n",
    "    sampler = WeightedRandomSampler(weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "                                    num_samples=len(train_idx),\n",
    "                                    replacement=True)\n",
    "\n",
    "    train_subset = torch.utils.data.Subset(ds_full, train_idx.tolist())\n",
    "    val_subset   = torch.utils.data.Subset(ds_full, val_idx.tolist())\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    val_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def evaluate_and_save(model: nn.Module, val_loader: DataLoader,\n",
    "                      idx2label: Dict[int, str], best_model_path: str, export_dir: str):\n",
    "    \"\"\"Evaluate model on val_loader and export in multiple formats.\"\"\"\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(DEVICE, non_blocking=False)\n",
    "            yb = yb.to(DEVICE, non_blocking=False)\n",
    "            logits = model(xb)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_true.extend(yb.cpu().numpy().tolist())\n",
    "            y_pred.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.int64)\n",
    "    y_pred = np.array(y_pred, dtype=np.int64)\n",
    "\n",
    "    # Explicitly pass labels actually present (union) to avoid mismatch\n",
    "    labels_present = sorted(set(y_true.tolist()) | set(y_pred.tolist()))\n",
    "    target_names = [idx2label[i] for i in labels_present]\n",
    "\n",
    "    print(\"\\nValidation report (best checkpoint):\")\n",
    "    print(classification_report(y_true, y_pred, labels=labels_present,\n",
    "                                target_names=target_names, zero_division=0))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "    print(confusion_matrix(y_true, y_pred, labels=labels_present))\n",
    "\n",
    "    # Ensure export dir\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "    # Also save a final PyTorch weights copy\n",
    "    final_pt = os.path.join(export_dir, \"mosquito_cnn_final.pt\")\n",
    "    torch.save(model.state_dict(), final_pt)\n",
    "\n",
    "    # TorchScript\n",
    "    example_input, _ = next(iter(val_loader))\n",
    "    example_input = example_input.to(\"cpu\")\n",
    "    ts = torch.jit.trace(model.cpu(), example_input)\n",
    "    ts_path = os.path.join(export_dir, \"mosquito_cnn_final_torchscript.pt\")\n",
    "    ts.save(ts_path)\n",
    "\n",
    "    # ONNX\n",
    "    onnx_path = os.path.join(export_dir, \"mosquito_cnn_final.onnx\")\n",
    "    torch.onnx.export(\n",
    "        model.cpu(),\n",
    "        example_input,\n",
    "        onnx_path,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "        opset_version=11\n",
    "    )\n",
    "    print(f\"\\nâœ… Models exported to:\\n  - {final_pt}\\n  - {ts_path}\\n  - {onnx_path}\")\n",
    "\n",
    "def train():\n",
    "    # Load metadata CSV\n",
    "    if not _safe_exists(CSV_PATH):\n",
    "        raise FileNotFoundError(f\"CSV_PATH not found: {CSV_PATH}\")\n",
    "    df_all = pd.read_csv(CSV_PATH)\n",
    "    required_cols = {\"mix_file\", \"species\"}\n",
    "    missing = required_cols - set([c.lower() for c in df_all.columns.str.lower()])\n",
    "    # Try to normalize column names if needed\n",
    "    df_all.columns = [c.strip() for c in df_all.columns]\n",
    "    # remap columns case-insensitively\n",
    "    rename_map = {}\n",
    "    for col in df_all.columns:\n",
    "        cl = col.lower()\n",
    "        if cl == \"mix_file\":\n",
    "            rename_map[col] = \"mix_file\"\n",
    "        elif cl == \"species\":\n",
    "            rename_map[col] = \"species\"\n",
    "    if rename_map:\n",
    "        df_all = df_all.rename(columns=rename_map)\n",
    "    assert \"mix_file\" in df_all.columns and \"species\" in df_all.columns, \\\n",
    "        \"CSV must contain 'mix_file' and 'species' columns.\"\n",
    "\n",
    "    # Build datasets\n",
    "    full_ds, mixed_ds, label2idx, idx2label = make_datasets(df_all)\n",
    "\n",
    "    # Labels for split\n",
    "    labels_all = indices_and_labels_from_concat(full_ds)\n",
    "\n",
    "    # Split\n",
    "    train_idx, val_idx = stratified_split_indices(labels_all, VAL_SIZE)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader, val_loader = make_loaders(full_ds, train_idx, val_idx, labels_all)\n",
    "\n",
    "    # Model\n",
    "    n_classes = len(label2idx)\n",
    "    print(f\"Classes ({n_classes}): { {i: lbl for i, lbl in idx2label.items()} }\")\n",
    "    print(f\"Training samples: {len(train_idx)} | Validation: {len(val_idx)}\")\n",
    "    model = TinyCNN(n_classes).to(DEVICE)\n",
    "\n",
    "    # Loss with class weights (inverse frequency from train set)\n",
    "    y_train = labels_all[train_idx]\n",
    "    class_counts = np.bincount(y_train, minlength=n_classes)\n",
    "    class_w = 1.0 / (class_counts + 1e-6)\n",
    "    class_w = class_w / class_w.mean()\n",
    "    criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_w, dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    # Remove verbose kwarg for compatibility with older PyTorch versions\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)\n",
    "\n",
    "    # Training loop with early stopping\n",
    "    best_acc = -1.0\n",
    "    patience = PATIENCE\n",
    "    best_path = os.path.join(MODEL_DIR, \"mosquito_cnn_best.pt\")\n",
    "\n",
    "    print(\"\\nStarting training on CPU...\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"[Epoch {epoch:02d}] Train\", leave=False):\n",
    "            xb = xb.to(DEVICE, non_blocking=False)\n",
    "            yb = yb.to(DEVICE, non_blocking=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "        train_loss = running_loss / max(1, total)\n",
    "        train_acc = correct / max(1, total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in tqdm(val_loader, desc=f\"[Epoch {epoch:02d}] Val  \", leave=False):\n",
    "                xb = xb.to(DEVICE, non_blocking=False)\n",
    "                yb = yb.to(DEVICE, non_blocking=False)\n",
    "\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        val_loss = val_loss / max(1, val_total)\n",
    "        val_acc = val_correct / max(1, val_total)\n",
    "\n",
    "        print(f\"[Epoch {epoch:02d}] train_loss={train_loss:.4f} acc={train_acc:.4f} | val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "\n",
    "        # Scheduler + Early stopping\n",
    "        scheduler.step(val_acc)\n",
    "        # manual LR log for compatibility across PyTorch versions\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"    LR adjusted to: {param_group['lr']:.6f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  âœ… Saved best checkpoint to: {best_path}\")\n",
    "            patience = PATIENCE\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load best checkpoint for final eval/export\n",
    "    model.load_state_dict(torch.load(best_path, map_location=\"cpu\"))\n",
    "    evaluate_and_save(model, val_loader, idx2label, best_model_path=best_path, export_dir=EXPORT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5f12a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edfb3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded successfully!\n",
      "ðŸŽ¤ Listening... Press Ctrl+C to stop.\n",
      "ðŸ”Š Detected: an sinensis  (confidence: 0.14)\n",
      "ðŸ”Š Detected: an sinensis  (confidence: 0.14)\n",
      "ðŸ”Š Detected: an sinensis  (confidence: 0.14)\n",
      "\n",
      "ðŸ›‘ Stopped recording.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# ========== Load Model ==========\n",
    "model_path = r\"mosquito_cnn_final_torchscript.pt\"\n",
    "model = torch.jit.load(model_path, map_location=\"cpu\")\n",
    "model.eval()\n",
    "print(\"âœ… Model loaded successfully!\")\n",
    "\n",
    "# Species index mapping\n",
    "species_index_map = {\n",
    "    0: 'no_mosquito', 1: 'ae aegypti', 2: 'ae albopictus', 3: 'an albimanus',\n",
    "    4: 'an arabiensis', 5: 'an atroparvus', 6: 'an barbirostris', 7: 'an coluzzii',\n",
    "    8: 'an coustani', 9: 'an dirus', 10: 'an farauti', 11: 'an freeborni',\n",
    "    12: 'an funestus', 13: 'an funestus sl', 14: 'an funestus ss', 15: 'an gambiae',\n",
    "    16: 'an gambiae sl', 17: 'an gambiae ss', 18: 'an harrisoni', 19: 'an leesoni',\n",
    "    20: 'an maculatus', 21: 'an maculipalpis', 22: 'an merus', 23: 'an minimus',\n",
    "    24: 'an pharoensis', 25: 'an quadriannulatus', 26: 'an rivulorum', 27: 'an sinensis',\n",
    "    28: 'an squamosus', 29: 'an stephensi', 30: 'an ziemanni', 31: 'coquillettidia sp',\n",
    "    32: 'culex pipiens complex', 33: 'culex quinquefasciatus', 34: 'culex tarsalis',\n",
    "    35: 'culex tigripes', 36: 'ma africanus', 37: 'ma uniformis', 38: 'toxorhynchites brevipalpis'\n",
    "}\n",
    "\n",
    "# ========== Audio Recording Settings ==========\n",
    "SAMPLE_RATE = 16000   # Hz\n",
    "DURATION = 1          # seconds (record in chunks of 1 second)\n",
    "\n",
    "# ========== Transform to Spectrogram ==========\n",
    "transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=64,\n",
    "    n_fft=1024,\n",
    "    hop_length=512\n",
    ")\n",
    "\n",
    "def predict_from_audio(audio_chunk):\n",
    "    \"\"\"Convert raw audio -> mel spectrogram -> model prediction\"\"\"\n",
    "    # Convert to tensor\n",
    "    audio_tensor = torch.from_numpy(audio_chunk).float()\n",
    "    \n",
    "    # If stereo, take mean (convert to mono)\n",
    "    if audio_tensor.ndim > 1:\n",
    "        audio_tensor = audio_tensor.mean(dim=1)\n",
    "    \n",
    "    # Convert to spectrogram (shape: [n_mels, time])\n",
    "    spec = transform(audio_tensor)\n",
    "    \n",
    "    # Resize/Pad/Crop to match training input size (64x64 assumed)\n",
    "    spec = torch.nn.functional.interpolate(spec.unsqueeze(0).unsqueeze(0),\n",
    "                                           size=(64, 64),\n",
    "                                           mode=\"bilinear\",\n",
    "                                           align_corners=False)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(spec)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return predicted_class, probs[0, predicted_class].item()\n",
    "\n",
    "# ========== Live Loop ==========\n",
    "print(\"ðŸŽ¤ Listening... Press Ctrl+C to stop.\")\n",
    "try:\n",
    "    while True:\n",
    "        # Record 1 sec of audio\n",
    "        audio = sd.rec(int(SAMPLE_RATE * DURATION), samplerate=SAMPLE_RATE, channels=1, dtype='float32')\n",
    "        sd.wait()\n",
    "\n",
    "        audio = np.squeeze(audio)  # remove channel dim\n",
    "\n",
    "        # Predict species\n",
    "        pred_class, confidence = predict_from_audio(audio)\n",
    "        species_name = species_index_map[pred_class]\n",
    "\n",
    "        print(f\"ðŸ”Š Detected: {species_name}  (confidence: {confidence:.2f})\")\n",
    "        time.sleep(0.01)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nðŸ›‘ Stopped recording.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_stable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
